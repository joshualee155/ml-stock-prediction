{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random\n",
    "import math \n",
    "from collections import deque\n",
    "import gym\n",
    "import trading_env\n",
    "import datetime\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1823, 0.0011, 0.1685, 0.1584, 0.0353, 0.0116, 0.0603, 0.1194, 0.0552,\n",
       "        0.0384, 0.1925, 0.0722, 0.1355, 0.0838, 0.1881, 0.0849],\n",
       "       grad_fn=<StdBackward1>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.RNN(2, 16, 1, nonlinearity='relu', batch_first=True)\n",
    "x = torch.randn( 64, 100, 2 )\n",
    "y, _ = rnn(x, )\n",
    "y[:, -1, :].std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2459, 0.2060, 0.2549, 0.2907, 0.2015, 0.2075, 0.2868, 0.2838, 0.2824,\n",
       "        0.2549, 0.2592, 0.4063, 0.2579, 0.1398, 0.3846, 0.2637],\n",
       "       grad_fn=<StdBackward1>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.RNN(2, 16, 1, nonlinearity='tanh', batch_first=True)\n",
    "x = torch.randn( 64, 100, 2 )\n",
    "y, _ = rnn(x, )\n",
    "y[:, -1, :].std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.1250e-05, 1.6928e-04, 1.3342e-04, 5.8910e-05, 5.5610e-05, 1.2909e-04,\n",
       "        1.1839e-04, 1.5173e-04, 5.6734e-05, 4.0919e-05, 3.1744e-05, 8.9828e-05,\n",
       "        9.1484e-05, 1.0185e-04, 1.1654e-04, 1.7232e-04],\n",
       "       grad_fn=<StdBackward1>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = nn.LSTM(2, 16, 5, batch_first=True)\n",
    "x = torch.randn( 64, 100, 2 )\n",
    "y, _ = lstm(x, )\n",
    "y[:, -1, :].std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor():\n",
    "\n",
    "    def __init__(self, env, learning_rate = 0.0001, tau = 0.001, quiet = True):\n",
    "        \n",
    "        self.env = env\n",
    "        self._quiet = quiet\n",
    "        self._tau = tau\n",
    "\n",
    "        self.state_dim = np.prod(np.array(env.observation_space.shape))\n",
    "        \n",
    "        # Actor network\n",
    "        self.model = self.create_actor_network().to(device)\n",
    "        # Target network\n",
    "        self.target = self.create_actor_network().to(device)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr = learning_rate)\n",
    "\n",
    "        # syncronize actor and target\n",
    "        for var1, var2 in zip(self.model.parameters(), self.target.parameters()):\n",
    "            var2 = var1\n",
    "\n",
    "    def create_actor_network(self):\n",
    "        \n",
    "        # neural featurizer parameters\n",
    "        h1 = 256\n",
    "        h2 = 128\n",
    "        h3 = 128\n",
    "        \n",
    "        class LSTM_Last(nn.Module):\n",
    "            \n",
    "            def __init__(self, *args, **kwargs):\n",
    "                super(LSTM_Last, self).__init__()\n",
    "                self.lstm = nn.LSTM(*args, **kwargs)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                x, _ = self.lstm(x)\n",
    "                return x[:, -1, :]\n",
    "        \n",
    "        model = nn.Sequential(\n",
    "            LSTM_Last(2, h1, 2, batch_first=True),\n",
    "            nn.Linear(h1, h2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2, h3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h3, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        return model \n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.Tensor([state]).float().to(device)      \n",
    "        action = self.model(state).detach().cpu().item()\n",
    "        return action\n",
    "\n",
    "    def act_target(self, states):\n",
    "        # called at calculating q_targets\n",
    "        action = self.target(states)\n",
    "        return action\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for var1, var2 in zip(self.model.parameters(), self.target.parameters()):\n",
    "                var2 = self._tau * var1 + (1-self._tau) * var2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic():\n",
    "\n",
    "    def __init__(self, env, learning_rate = 0.001, tau = 0.001, quiet = True):\n",
    "        \n",
    "        self._env = env\n",
    "        self._quiet = quiet\n",
    "        self._tau = tau\n",
    "        \n",
    "        self._state_dim = np.prod(np.array(env.observation_space.shape))\n",
    "\n",
    "        # critic network\n",
    "        self.model = self.create_critic_network().to(device)\n",
    "\n",
    "        # critic target network\n",
    "        self.target = self.create_critic_network().to(device)\n",
    "\n",
    "        # optimizer only applies to model network\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr = learning_rate)\n",
    "        \n",
    "        # synchronize critic and target\n",
    "        with torch.no_grad():\n",
    "            for var1, var2 in zip(self.model.parameters(), self.target.parameters()):\n",
    "                var2 = var1\n",
    "        \n",
    "    def create_critic_network(self):\n",
    "        \n",
    "        # neural featurizer parameters\n",
    "        h1 = 256\n",
    "        h2 = 128\n",
    "        h3 = 128\n",
    "        \n",
    "        class LSTM_Last(nn.Module):\n",
    "            \n",
    "            def __init__(self, *args, **kwargs):\n",
    "                super(LSTM_Last, self).__init__()\n",
    "                self.lstm = nn.LSTM(*args, **kwargs)\n",
    "                self.model = nn.Sequential(\n",
    "                                nn.Linear(h1+1, h2),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(h2, h3),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(h3, 1),\n",
    "                            )\n",
    "            \n",
    "            def forward(self, x, a):\n",
    "                x, _ = self.lstm(x)\n",
    "                x = torch.cat([x[:,-1,:], a], 1)\n",
    "                return self.model(x)\n",
    "        \n",
    "        model = LSTM_Last(2, h1, 2, batch_first=True)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_q_values(self, states, actions):\n",
    "        \n",
    "        return self.model(states, actions).squeeze()\n",
    "\n",
    "    def get_target_q_values(self, states, actions):\n",
    "\n",
    "        return self.target(states, actions).squeeze()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for var1, var2 in zip(self.model.parameters(), self.target.parameters()):\n",
    "                var2 = self._tau * var1 + (1-self._tau) * var2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent():\n",
    "    def __init__(self, env, buffer_size = 1000000, discount_rate = 0.99, batch_size = 128, tau = 0.001, \n",
    "                 actor_lr = 1e-5, critic_lr = 1e-4, update_steps = 100,\n",
    "                 quiet = True):\n",
    "        \n",
    "        self.actor = Actor(env, learning_rate = actor_lr, quiet = quiet, tau = tau)\n",
    "        self.critic = Critic(env, learning_rate = critic_lr, quiet = quiet, tau = tau)\n",
    "        \n",
    "        self._batch_size = batch_size\n",
    "        self._discount_rate = discount_rate\n",
    "        self._update_steps = update_steps\n",
    "        \n",
    "        # Memory\n",
    "        self.memory = deque( maxlen = buffer_size )\n",
    "        \n",
    "    def get_gradient_norm(self, model):\n",
    "\n",
    "        total_norm = 0.0\n",
    "        for p in model.parameters():\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "\n",
    "        return total_norm\n",
    "\n",
    "    def store_step(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.memory.append([state, action, reward, next_state, done])\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        actor_grad = 0\n",
    "        critic_grad = 0\n",
    "        for _ in range(self._update_steps):\n",
    "            batch = random.sample(self.memory, self._batch_size)\n",
    "            \n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.Tensor(states).float().to(device)\n",
    "            actions = torch.Tensor(actions).float().to(device).unsqueeze(1)\n",
    "            rewards = torch.Tensor(rewards).float().to(device)\n",
    "            next_states = torch.Tensor(next_states).float().to(device)\n",
    "            dones = torch.Tensor(dones).float().to(device)\n",
    "\n",
    "            target_actions = self.actor.act_target(next_states)\n",
    "            q_targets = rewards + (1.0 - dones) * self._discount_rate * self.critic.get_target_q_values(next_states, target_actions)\n",
    "            \n",
    "            mse_loss = nn.MSELoss()\n",
    "            \n",
    "            self.critic.optimizer.zero_grad()\n",
    "            q_values = self.critic.get_q_values(states, actions)\n",
    "            critic_loss = mse_loss( q_values, q_targets )\n",
    "            critic_loss.backward()\n",
    "            critic_grad += self.get_gradient_norm(self.critic.model)\n",
    "            self.critic.optimizer.step()\n",
    "\n",
    "            # freeze critic network for actor update, to avoid unnecessary grad calculation\n",
    "            for p in self.critic.model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            self.actor.optimizer.zero_grad()\n",
    "            actor_loss = -self.critic.get_q_values( states, self.actor.model(states) ).mean()\n",
    "            # print(\"actor loss before update: {}\".format(actor_loss.item()))\n",
    "            actor_loss.backward()\n",
    "            actor_grad += self.get_gradient_norm(self.actor.model)\n",
    "            self.actor.optimizer.step()\n",
    "            # actor_loss = -self.critic.get_q_values( states, self.actor.model(states) ).mean()\n",
    "            # print(\"actor loss after update: {}\".format(actor_loss.item()))\n",
    "\n",
    "            # unfreeze critic network after actor update\n",
    "            for p in self.critic.model.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "            self.actor.update_target_network()\n",
    "            self.critic.update_target_network()\n",
    "        \n",
    "        return actor_grad / self._update_steps, critic_grad / self._update_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical data file\n"
     ]
    }
   ],
   "source": [
    "env_trading = gym.make('test_trading-v2')\n",
    "NUM_EP = 400\n",
    "\n",
    "agentDDPG = DDPGAgent(env_trading, \n",
    "                    buffer_size=1000000,\n",
    "                    tau = 0.01, \n",
    "                    actor_lr = 1e-4, \n",
    "                    critic_lr = 1e-4)\n",
    "\n",
    "# Ornstein-Uhlenbeck noise by lirnli/OpenAI-gym-solutions    \n",
    "def UONoise():\n",
    "    theta = 0.15\n",
    "    sigma = 0.2\n",
    "    state = 0\n",
    "    while True:\n",
    "        yield state\n",
    "        state += -theta*state+sigma*np.random.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env_trading.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating memory buffer: 11520/10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date = datetime.datetime(2017, 7, 15, 0, 0)\n",
    "date_test = datetime.datetime(2017, 7, 15, 0, 0)\n",
    "noise = UONoise()\n",
    "scores = []\n",
    "scores_test = []\n",
    "sample_actions = [] # Keep track of actions every 100 episode\n",
    "portfolios = []\n",
    "actor_grads = []\n",
    "critic_grads = []\n",
    "\n",
    "while (len(agentDDPG.memory) < 10000):\n",
    "    state = env_trading.reset(date = date)\n",
    "#     state = np.reshape(state,200)\n",
    "    while (True):\n",
    "        # action = agentDDPG.actor.act(state)\n",
    "        # action = np.clip( action + next(noise), -1, 1 )\n",
    "        action = env_trading.action_space.sample()[0]\n",
    "        next_state, reward, done, _ = env_trading.step(action)\n",
    "#         state = state.reshape(200)\n",
    "#         next_state = next_state.reshape(200)\n",
    "        agentDDPG.store_step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        print(\"\\rPopulating memory buffer: {:5d}/10000\".format(len(agentDDPG.memory)), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3, Training reward: 1.30, Testing reward: 0.09, Actor grad: 0.0068, Critic grad: 0.0791, Actions: 0.0318+/-0.0000, Test Actions: 0.2360+/-0.0000000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-c2b2623dfa5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mactor_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magentDDPG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mactor_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mcritic_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-deb215575305>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mactor_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_q_values\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;31m# print(\"actor loss before update: {}\".format(actor_loss.item()))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mactor_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m             \u001b[0mactor_grad\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_gradient_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\torch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(NUM_EP):\n",
    "#     state = np.reshape(env_trading.reset(date=date), 200)\n",
    "    score = 0\n",
    "\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    while(True):\n",
    "        action = agentDDPG.actor.act(state)\n",
    "        action += next( noise )\n",
    "        action = np.clip(action, -1, 1)\n",
    "        actions.append(action)\n",
    "        next_state, reward, done, _ = env_trading.step( action )\n",
    "#         next_state = np.reshape(next_state, 200)\n",
    "        score += reward\n",
    "        rewards.append( reward )\n",
    "\n",
    "        agentDDPG.store_step(state, action, reward, next_state, done)\n",
    "\n",
    "        if done:\n",
    "            actor_grad, critic_grad = agentDDPG.train()\n",
    "            actor_grads.append(actor_grad)\n",
    "            critic_grads.append(critic_grad)\n",
    "            scores.append(score)\n",
    "            # print(\"Episode: {}, Total reward: {}\".format(e, score))\n",
    "            break\n",
    "        state = next_state\n",
    "\n",
    "    # Testing session\n",
    "#     state = np.reshape(env_trading.reset( date = date_test ), 200)\n",
    "    score_test = 0\n",
    "    actions_test = []\n",
    "    while(True):\n",
    "        action = agentDDPG.actor.act(state)\n",
    "        next_state, reward, done, _ = env_trading.step( action )\n",
    "        actions_test.append( action )\n",
    "#         next_state = np.reshape(next_state, 200)\n",
    "        score_test += reward\n",
    "        if done:\n",
    "            # agentDDPG.actor.update_averages( rewards, [score_test] )\n",
    "            # agentDDPG.actor.record_summary( e )\n",
    "            scores_test.append(score_test)\n",
    "            portfolios.append( env_trading.portfolio_value )\n",
    "            if e % 100 == 0:\n",
    "                sample_actions.append( actions_test )\n",
    "            print(\"\\rEpisode: {}, Training reward: {:.2f}, Testing reward: {:.2f}, Actor grad: {:.4f}, Critic grad: {:.4f}, Actions: {:.4f}+/-{:.4f}, Test Actions: {:.4f}+/-{:.4f}\".format(e, score, score_test, actor_grad, critic_grad, np.mean(actions), np.std(actions), np.mean(actions_test), np.std(actions_test)), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            break\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = random.sample(agentDDPG.memory, 64)\n",
    "            \n",
    "states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "states = torch.Tensor(states).float().to(device)\n",
    "actions = torch.Tensor(actions).float().to(device).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-aed6f3e69174>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0magentDDPG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'a'"
     ]
    }
   ],
   "source": [
    "agentDDPG.critic.model((states, actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('torch': conda)",
   "language": "python",
   "name": "python38264bittorchconda2b113012fcee40479eb88836a0f53f51"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
