{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, state_dim):  \n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        h1 = 512\n",
    "        h2 = 256\n",
    "        h3 = 128\n",
    "        \n",
    "        self.hidden_1 = nn.Linear(state_dim, h1)\n",
    "        self.hidden_2 = nn.Linear(h1, h2)\n",
    "        self.hidden_3 = nn.Linear(h2, h3)\n",
    "\n",
    "        self.output = nn.Linear(h3, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden_1(x))\n",
    "        x = F.relu(self.hidden_2(x))\n",
    "        x = F.relu(self.hidden_3(x))\n",
    "        return self.output(x)\n",
    "\n",
    "class DQNAgent():\n",
    "    def __init__(self, env, gamma=0.99, buffer_size = 1000000,\n",
    "        epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.999, \n",
    "        alpha=1e-4, alpha_decay=0.001, batch_size=128, quiet=False):\n",
    "        \n",
    "        self.env = env\n",
    "        self.memory = deque(maxlen = buffer_size)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self._batch_size = batch_size\n",
    "        self.quiet = quiet\n",
    "        self._state_dim = np.prod(np.array(env.observation_space.shape))\n",
    "        \n",
    "        self.model = Model(self._state_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.alpha)\n",
    "\n",
    "    def store_step(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, step = None):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        if step is not None:\n",
    "            epsilon = max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((step + 1) * self.epsilon_decay)))\n",
    "            q_values = self.model(state)\n",
    "            q_values = q_values.detach().cpu().numpy()\n",
    "            random_action = np.random.choice(3) - 1 # action space: -1, 0, 1\n",
    "            q_max_actrion = np.argmax(q_values) - 1\n",
    "            action = random_action if (np.random.random() <= epsilon) else q_max_action\n",
    "            return action\n",
    "            print(\"here\")\n",
    "        else:\n",
    "            q_values = self.model(state)\n",
    "            q_values = q_values.detach().cpu().numpy()\n",
    "            return np.argmax(q_values) - 1\n",
    "\n",
    "    def train(self):\n",
    "        batch_size = self._batch_size\n",
    "        x_batch, y_batch = [], []\n",
    "        \n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(states).float().to(device)\n",
    "        actions = torch.tensor(actions).long().to(device)\n",
    "        rewards = torch.tensor(rewards).float().to(device)\n",
    "        next_states = torch.tensor(next_states).float().to(device)\n",
    "        dones = torch.tensor(dones).float().to(device)\n",
    "\n",
    "        y_preds = self.model(states)\n",
    "        actions = actions.unsqueeze(1)\n",
    "        y_preds = y_preds.gather(1, actions)\n",
    "        \n",
    "        y_targets = rewards + (1 - dones) * self.gamma * self.model(next_states).detach().max(1)[0]\n",
    "\n",
    "        loss = torch.mean( (y_preds - y_targets)**2 )\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        # self.memory = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "\n",
    "import gym\n",
    "import trading_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical data file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.02819104, 0.32829856],\n",
       "       [1.03334742, 0.6872087 ],\n",
       "       [1.04243449, 1.04861368],\n",
       "       [1.04057481, 0.68456713],\n",
       "       [1.04049028, 0.43732447],\n",
       "       [1.03964497, 0.53619503],\n",
       "       [1.03393914, 0.36730053],\n",
       "       [1.03516484, 0.68563439],\n",
       "       [1.03989856, 0.41589084],\n",
       "       [1.04370245, 0.72421409],\n",
       "       [1.04387151, 0.3563747 ],\n",
       "       [1.04361792, 0.15185302],\n",
       "       [1.04357566, 0.32994251],\n",
       "       [1.04361792, 0.5751208 ],\n",
       "       [1.04366019, 0.30587477],\n",
       "       [1.04801352, 0.73526962],\n",
       "       [1.05418428, 0.92314449],\n",
       "       [1.04615385, 0.60235922],\n",
       "       [1.05071851, 0.41567612],\n",
       "       [1.05879121, 1.14874656],\n",
       "       [1.0658918 , 0.62194044],\n",
       "       [1.05350803, 2.02918238],\n",
       "       [1.05528318, 0.76909247],\n",
       "       [1.05663567, 0.62731009],\n",
       "       [1.05164835, 1.0132689 ],\n",
       "       [1.05126796, 0.44822558],\n",
       "       [1.04945055, 0.70310004],\n",
       "       [1.04268808, 0.67092055],\n",
       "       [1.04556213, 0.38741467],\n",
       "       [1.04797126, 0.17656399],\n",
       "       [1.05283178, 0.29523912],\n",
       "       [1.05291631, 0.24117453],\n",
       "       [1.05397295, 0.08836332],\n",
       "       [1.05397295, 0.14707188],\n",
       "       [1.04716822, 0.17209288],\n",
       "       [1.04606932, 0.32651019],\n",
       "       [1.04027895, 0.096648  ],\n",
       "       [1.03981403, 0.10523649],\n",
       "       [1.03558749, 0.40004082],\n",
       "       [1.0353339 , 0.1304355 ],\n",
       "       [1.03499577, 0.11150859],\n",
       "       [1.0350803 , 0.1401556 ],\n",
       "       [1.03736264, 0.05063325],\n",
       "       [1.03478445, 0.06166359],\n",
       "       [1.04247675, 0.07399901],\n",
       "       [1.04687236, 0.12017006],\n",
       "       [1.03994083, 0.05870894],\n",
       "       [1.03558749, 0.10200156],\n",
       "       [1.0341082 , 0.26852717],\n",
       "       [1.04344886, 0.08591778],\n",
       "       [1.04002536, 0.11094624],\n",
       "       [1.03791209, 0.24693741],\n",
       "       [1.03601014, 0.09645616],\n",
       "       [1.03546069, 0.20508012],\n",
       "       [1.03224852, 0.08572942],\n",
       "       [1.03241758, 0.34698038],\n",
       "       [1.02409129, 0.93284417],\n",
       "       [1.0228656 , 0.36399353],\n",
       "       [1.02544379, 0.32784944],\n",
       "       [1.02210482, 0.45658911],\n",
       "       [1.02489434, 0.44581661],\n",
       "       [1.0249366 , 0.20230466],\n",
       "       [1.0237109 , 0.24326271],\n",
       "       [1.0249366 , 0.20660671],\n",
       "       [1.03119189, 0.68760111],\n",
       "       [1.03254438, 0.38513479],\n",
       "       [1.03127642, 0.34658177],\n",
       "       [1.02688081, 0.26019933],\n",
       "       [1.02717667, 0.58724533],\n",
       "       [1.02476754, 0.26595255],\n",
       "       [1.02599324, 0.31018877],\n",
       "       [1.02379544, 0.43892903],\n",
       "       [1.02683855, 0.35338806],\n",
       "       [1.02641589, 0.2838743 ],\n",
       "       [1.0260355 , 0.19623969],\n",
       "       [1.02333052, 0.3412441 ],\n",
       "       [1.02345731, 0.2797468 ],\n",
       "       [1.02349958, 0.30139091],\n",
       "       [1.02434489, 0.28287234],\n",
       "       [1.0238377 , 0.1933677 ],\n",
       "       [1.02333052, 0.25460974],\n",
       "       [1.02459848, 0.38211873],\n",
       "       [1.02730347, 0.88254634],\n",
       "       [1.02730347, 0.38093825],\n",
       "       [1.0271344 , 0.69094057],\n",
       "       [1.02650042, 0.21520104],\n",
       "       [1.02709214, 0.50998708],\n",
       "       [1.02789518, 0.12902389],\n",
       "       [1.02645816, 0.203766  ],\n",
       "       [1.02645816, 0.20119176],\n",
       "       [1.02696534, 0.10391217],\n",
       "       [1.02413356, 0.44432432],\n",
       "       [1.02666948, 0.18022231],\n",
       "       [1.02324598, 0.35897354],\n",
       "       [1.02223161, 0.7146096 ],\n",
       "       [1.013694  , 0.66913542],\n",
       "       [1.01060862, 0.68931956],\n",
       "       [1.0067202 , 0.52130631],\n",
       "       [1.00934066, 2.62027655],\n",
       "       [1.        , 1.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_trading = gym.make('test_trading-v2')\n",
    "NUM_EP = 400\n",
    "date = datetime.datetime(2017, 7, 10, 0, 0)\n",
    "data = env_trading.historical_data[\"close\"]\n",
    "env_trading.reset(date=date)\n",
    "# plt.plot(data[env_trading.start_index:env_trading.start_index + int(env_trading.episode_steps) \n",
    "#             if env_trading.start_index + int(env_trading.episode_steps) < data.shape[0]\n",
    "#             else data.shape[0]])\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-c48b082e4009>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0magentDQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0magentDQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0magentDQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mstate_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_trading\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2017\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-59-91b5cc25293f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0my_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_preds\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "agentDQN = DQNAgent(env_trading, gamma=0.99, buffer_size = 10000,\n",
    "                    epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.999, \n",
    "                    alpha=1e-4, alpha_decay=0.001, batch_size=8, quiet=False)\n",
    "\n",
    "rewards = []\n",
    "rewards_test = []\n",
    "portfolio = []\n",
    "for i in range( NUM_EP ):\n",
    "    state = env_trading.reset(date = datetime.datetime( 2017, 7, 15, 0, 0 ))\n",
    "    state = np.reshape(state,200)\n",
    "    total_reward = 0\n",
    "\n",
    "    while(True):\n",
    "        action = agentDQN.act(state, step=i)\n",
    "#         print(action)\n",
    "        next_state, reward, done, _ = env_trading.step(action)\n",
    "        state = np.reshape(state,200)\n",
    "        next_state = next_state.reshape(200)\n",
    "        agentDQN.store_step(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            rewards.append(total_reward)\n",
    "#             portfolio.append(env_trading.portfolio_value)\n",
    "            #print(\"Episode: {}, Total reward: {}\".format(i,total_reward))\n",
    "            break\n",
    "    if len( agentDQN.memory ) > agentDQN._batch_size:\n",
    "        agentDQN.train()\n",
    "\n",
    "    state_test = env_trading.reset( date = datetime.datetime(2017, 8, 15, 0, 0) )\n",
    "    state_test = np.reshape( state_test, 200 )\n",
    "    total_reward_test = 0\n",
    "\n",
    "    while( True ):\n",
    "        action = agentDQN.act(state_test)\n",
    "#         print(action)\n",
    "        state_test, reward_test, done_test, _ = env_trading.step(action)\n",
    "        state_test = np.reshape(state_test,200)\n",
    "        total_reward_test += reward_test\n",
    "        if done_test:\n",
    "            rewards_test.append(total_reward_test)\n",
    "            portfolio.append(env_trading.portfolio_value)\n",
    "            print(\"Episode: {}, Training reward: {}, Testing reward: {}\".format(i, total_reward, total_reward_test))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dqn_log.txt', 'r') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Episode: 0, Training reward: 32.641549267868896, Testing reward: 7.8558623343574725\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('torch': conda)",
   "language": "python",
   "name": "python38264bittorchconda2b113012fcee40479eb88836a0f53f51"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
